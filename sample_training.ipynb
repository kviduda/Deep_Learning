{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da28f396-ed01-49de-9c34-2eeccd38f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from scipy.special import expit\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def data_preprocess():\n",
    "    filepath = 'data/'\n",
    "    with open(filepath + 'training_label.json', 'r') as f:\n",
    "        file = json.load(f)\n",
    "\n",
    "    wo_co = {}\n",
    "    for d in file:\n",
    "        for s in d['caption']:\n",
    "            wor_sen = re.sub('[.!,;?]', ' ', s).split()\n",
    "            for word in wor_sen:\n",
    "                word = word.replace('.', '') if '.' in word else word\n",
    "                if word in wo_co:\n",
    "                    wo_co[word] += 1\n",
    "                else:\n",
    "                    wo_co[word] = 1\n",
    "\n",
    "    word_dict = {}\n",
    "    for word in wo_co:\n",
    "        if wo_co[word] > 4:\n",
    "            word_dict[word] = wo_co[word]\n",
    "\n",
    "    use_tok = [('<PAD>', 0), ('<SOS>', 1), ('<EOS>', 2), ('<UNK>', 3)]\n",
    "    i2w = {}\n",
    "    w2i = {}\n",
    "    for i, w in enumerate(word_dict):\n",
    "        i2w[i + len(use_tok)] = w\n",
    "        w2i[w] = i + len(use_tok)\n",
    "\n",
    "    for token, index in use_tok:\n",
    "        i2w[index] = token\n",
    "        w2i[token] = index\n",
    "\n",
    "    return i2w, w2i, word_dict\n",
    "\n",
    "def s_split(sentence, word_dict, w2i):\n",
    "    sentence = [w2i[word] if word in word_dict else 3 for word in re.sub(r'[.!,;?]', ' ', sentence).split()]\n",
    "    sentence.insert(0, 1)  # Adding SOS token at the beginning\n",
    "    sentence.append(2)  # Adding EOS token at the end\n",
    "    return sentence\n",
    "\n",
    "def annotate(label_file, word_dict, w2i):\n",
    "    lab_json = 'data/' + label_file\n",
    "    caption = []\n",
    "    with open(lab_json, 'r') as f:\n",
    "        label = json.load(f)\n",
    "    for d in label:\n",
    "        for s in d['caption']:\n",
    "            s = s_split(s, word_dict, w2i)\n",
    "            caption.append((d['id'], s))\n",
    "    return caption\n",
    "\n",
    "def avi(files_dir):\n",
    "    avi_data = {}\n",
    "    training_feats = 'data/' + files_dir\n",
    "    files = os.listdir(training_feats)\n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "        print(i)\n",
    "        value = np.load(os.path.join(training_feats, file))\n",
    "        avi_data[file.split('.npy')[0]] = value\n",
    "    \n",
    "    return avi_data\n",
    "\n",
    "def minibatch(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    avi_data, captions = zip(*data) \n",
    "    avi_data = torch.stack(avi_data, 0)\n",
    "\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return avi_data, targets, lengths\n",
    "\n",
    "class training_data(Dataset):\n",
    "    def __init__(self, label_file, files_dir, word_dict, w2i):\n",
    "        self.label_file = label_file\n",
    "        self.word_dict = word_dict\n",
    "        self.files_dir = files_dir\n",
    "        self.avi = avi(label_file)\n",
    "        self.w2i = w2i\n",
    "        self.data_pair = annotate(files_dir, word_dict, w2i)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_pair)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert (idx < self.__len__())\n",
    "        avi_file_name, sentence = self.data_pair[idx]\n",
    "        data = torch.Tensor(self.avi[avi_file_name])\n",
    "        data += torch.Tensor(data.size()).random_(0, 2000)/10000.\n",
    "        return torch.Tensor(data), torch.Tensor(sentence)\n",
    "\n",
    "class lstm_attn_decode(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, vocab_size, word_dim, dropout_percentage=0.35):\n",
    "        super(lstm_attn_decode, self).__init__()\n",
    "\n",
    "        self.hidden_size = 512\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_dim = word_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, 1024)\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        self.lstm = nn.LSTM(hidden_size+word_dim, hidden_size, batch_first=True)\n",
    "        self.attention = attention(hidden_size)\n",
    "        self.to_final_output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_last_hidden_state, encoder_output, targets=None, mode='train', tr_steps=None):\n",
    "        _, batch_size, _ = encoder_last_hidden_state.size()\n",
    "        \n",
    "        decoder_current_hidden_state = None if encoder_last_hidden_state is None else encoder_last_hidden_state\n",
    "        decoder_cxt = torch.zeros(decoder_current_hidden_state.size())\n",
    "\n",
    "        decoder_current_input_word = Variable(torch.ones(batch_size, 1)).long()\n",
    "        seq_logProb = []\n",
    "        seq_predictions = []\n",
    "\n",
    "        targets = self.embedding(targets)\n",
    "        _, seq_len, _ = targets.size()\n",
    "\n",
    "        i = 0\n",
    "        while i < seq_len - 1:\n",
    "            threshold = self.teacher_forcing_ratio(training_steps=tr_steps)\n",
    "            if random.uniform(0.05, 0.995) > threshold: \n",
    "                current_input_word = targets[:, i]  \n",
    "            else: \n",
    "                current_input_word = self.embedding(decoder_current_input_word).squeeze(1)\n",
    "\n",
    "            context = self.attention(decoder_current_hidden_state, encoder_output)\n",
    "            lstm_input = torch.cat([current_input_word, context], dim=1).unsqueeze(1)\n",
    "            lstm_output, t = self.lstm(lstm_input, (decoder_current_hidden_state, decoder_cxt))\n",
    "            decoder_current_hidden_state = t[0]\n",
    "            logprob = self.to_final_output(lstm_output.squeeze(1))\n",
    "            seq_logProb.append(logprob.unsqueeze(1))\n",
    "            decoder_current_input_word = logprob.unsqueeze(1).max(2)[1]\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        seq_logProb = torch.cat(seq_logProb, dim=1)\n",
    "        seq_predictions = seq_logProb.max(2)[1]\n",
    "        return seq_logProb, seq_predictions\n",
    "\n",
    "    def infer(self, encoder_last_hidden_state, encoder_output):\n",
    "        _, batch_size, _ = encoder_last_hidden_state.size()\n",
    "        decoder_current_hidden_state = None if encoder_last_hidden_state is None else encoder_last_hidden_state\n",
    "        decoder_current_input_word = Variable(torch.ones(batch_size, 1)).long()\n",
    "        decoder_c\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
